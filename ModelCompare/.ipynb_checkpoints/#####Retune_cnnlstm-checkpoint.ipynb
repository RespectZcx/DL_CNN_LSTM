{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michael.zhang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/michael.zhang/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/michael.zhang/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import six\n",
    "from six.moves import cPickle as cpik\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from keras.layers.recurrent import LSTM,GRU\n",
    "from keras import backend as K\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "import six\n",
    "from six.moves import cPickle as cpik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dl_dataset_by_ts(df, ts):\n",
    "    '''\n",
    "    Build up a (data_size * 11 * 7 * 7 * 2) dataset for both traning and testing\n",
    "    \n",
    "    Input\n",
    "    df: a dateframe\n",
    "    ts: time_steps, use how many previous time frame to predict the next one, e.g., if it is 11, then use the\n",
    "        previous 11 date to predict current 1.\n",
    "    \n",
    "    Output\n",
    "    Training set and testing set\n",
    "    \n",
    "    '''\n",
    "    fids = df['FishnetID'].unique().tolist()\n",
    "    #print(fids)\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for fid in fids:\n",
    "        df_current = df[df['FishnetID'] == fid].reset_index(drop = True) #original dataset has been ordered by time.\n",
    "        df_current_len = df_current.shape[0] # overall length of current data \n",
    "        if (df_current_len - ts - 1)>0:\n",
    "            for i in range(df_current_len - ts - 1):\n",
    "                # current slice of input data\n",
    "                X_train_cur = []\n",
    "                Y_train_cur = []\n",
    "                # append every X by time steps\n",
    "                for x_v in df_current['neighbours_index_image'].iloc[i : (i+ts)].values:\n",
    "                    X_train_cur.append(x_v)\n",
    "                    #print(np.array(X_cur).shape)    \n",
    "                X_train.append(np.array(X_train_cur))  \n",
    "                # append very Y by time steps\n",
    "                for y_v in df_current['PerformanceTarget'].iloc[i+1 : (i+ts+1)].values:\n",
    "                    Y_train_cur.append(y_v)\n",
    "                Y_train.append(np.array(Y_train_cur))\n",
    "            \n",
    "            for i in range(df_current_len - ts - 1, df_current_len - ts):\n",
    "                # current slice of input data\n",
    "                X_test_cur = []\n",
    "                Y_test_cur = []\n",
    "                # append every X by time steps\n",
    "                for x_v in df_current['neighbours_index_image'].iloc[i : (i+ts)].values:\n",
    "                    X_test_cur.append(x_v)\n",
    "                    #print(np.array(X_cur).shape)\n",
    "                X_test.append(np.array(X_test_cur))  \n",
    "                # append very Y by time steps\n",
    "                for y_v in df_current['PerformanceTarget'].iloc[i+1 : ].values:\n",
    "                    Y_test_cur.append(y_v)\n",
    "                Y_test.append(np.array(Y_test_cur))\n",
    "        else:\n",
    "            for i in range(df_current_len - ts):\n",
    "                # current slice of input data\n",
    "                X_cur = []\n",
    "                Y_cur = []\n",
    "                # append every X by time steps\n",
    "                for x_v in df_current['neighbours_index_image'].iloc[i : (i+ts)].values:\n",
    "                    X_cur.append(x_v)\n",
    "                    #print(np.array(X_cur).shape)    \n",
    "                X_train.append(np.array(X_cur))\n",
    "                X_test.append(np.array(X_cur))  \n",
    "                # append very Y by time steps\n",
    "                for y_v in df_current['PerformanceTarget'].iloc[i+1 : (i+ts+1)].values:\n",
    "                    Y_cur.append(y_v)\n",
    "                Y_train.append(np.array(Y_cur))\n",
    "                Y_test.append(np.array(Y_cur))      \n",
    "    return (X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MAPE loss function self defined for keras\n",
    "def mape_loss(y_true, y_pred):\n",
    "    diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n",
    "                                            K.epsilon(),\n",
    "                                            None))\n",
    "    return 100. * K.mean(diff, axis=-1)\n",
    "\n",
    "\n",
    "# MAPE loss for calculate test result.\n",
    "def mape(y_p, y):\n",
    "    y[y == 0] = 0.00001\n",
    "    diff = np.abs((y_p - y) / y)\n",
    "    #print(diff)\n",
    "    return 100. * np.mean(diff)\n",
    "\n",
    "\n",
    "\n",
    "# RMSE loss for calculate test result.\n",
    "def rmse(y_p, y):\n",
    "    diff = np.sqrt((y_p - y)**2)# np.sum()\n",
    "    return np.mean(diff)\n",
    "\n",
    "\n",
    "def moving_avg(df, ts):\n",
    "    '''\n",
    "    Build up moving average model\n",
    "    \n",
    "    Input\n",
    "    df: a dateframe\n",
    "    ts: time_steps, use how many previous time frame to predict the next one, e.g., if it is 11, then use the\n",
    "        previous 11 date to predict current 1.\n",
    "    \n",
    "    Output\n",
    "    Error of MAPE and RMSE\n",
    "    \n",
    "    '''\n",
    "    fids = df['FishnetID'].unique().tolist()\n",
    "    #print(fids)\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    for fid in fids:\n",
    "        df_current = df[df['FishnetID'] == fid].reset_index(drop = True) #original dataset has been ordered by time.\n",
    "        df_current_len = df_current.shape[0] # overall length of current data \n",
    "        if (df_current_len - ts - 1)>0:\n",
    "            for i in range(df_current_len - ts - 1):\n",
    "                # current slice of input data\n",
    "                X_train_cur = []\n",
    "                Y_train_cur = []\n",
    "                # append every X by time steps\n",
    "                for x_v in df_current['PerformanceTarget'].iloc[i : (i+ts)].values:\n",
    "                    X_train_cur.append(x_v)\n",
    "                    #print(np.array(X_cur).shape)\n",
    "                #print(X_train_cur)\n",
    "                X_train.append(np.array(np.mean(X_train_cur)))  \n",
    "                # append very Y by time steps\n",
    "                for y_v in df_current['PerformanceTarget'].iloc[i+1 : (i+ts+1)].values:\n",
    "                    Y_train_cur.append(y_v)\n",
    "                #print('Ytrain:  ', Y_train_cur)\n",
    "                Y_train.append(np.array(Y_train_cur[-1]))\n",
    "            \n",
    "            for i in range(df_current_len - ts - 1, df_current_len - ts):\n",
    "                # current slice of input data\n",
    "                X_test_cur = []\n",
    "                Y_test_cur = []\n",
    "                # append every X by time steps\n",
    "                for x_v in df_current['PerformanceTarget'].iloc[i : (i+ts)].values:\n",
    "                    X_test_cur.append(x_v)\n",
    "                    #print(np.array(X_cur).shape)\n",
    "                X_test.append(np.array(np.mean(X_test_cur)))  \n",
    "                # append very Y by time steps\n",
    "                for y_v in df_current['PerformanceTarget'].iloc[i+1 : ].values:\n",
    "                    Y_test_cur.append(y_v)\n",
    "                Y_test.append(np.array(Y_test_cur[-1]))\n",
    "        else:\n",
    "            for i in range(df_current_len - ts):\n",
    "                # current slice of input data\n",
    "                X_cur = []\n",
    "                Y_cur = []\n",
    "                # append every X by time steps\n",
    "                for x_v in df_current['PerformanceTarget'].iloc[i : (i+ts)].values:\n",
    "                    X_cur.append(x_v)\n",
    "                    #print(np.array(X_cur).shape)    \n",
    "                X_train.append(np.array(X_cur))\n",
    "                X_test.append(np.array(X_cur))  \n",
    "                # append very Y by time steps\n",
    "                for y_v in df_current['PerformanceTarget'].iloc[i+1 : (i+ts+1)].values:\n",
    "                    Y_cur.append(y_v)\n",
    "                Y_train.append(np.array(Y_cur[-1]))\n",
    "                Y_test.append(np.array(Y_cur[-1]))      \n",
    "    \n",
    "    \n",
    "    X_train = np.nan_to_num(np.array(X_train))\n",
    "    Y_train = np.nan_to_num(np.array(Y_train))\n",
    "    X_test = np.nan_to_num(np.array(X_test))\n",
    "    Y_test = np.nan_to_num(np.array(Y_test))\n",
    "    #print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)\n",
    "    #print((X_train - Y_train).shape)\n",
    "    traing_error_mape = mape(X_train, Y_train)\n",
    "    testing_error_mape = mape(X_test, Y_test)\n",
    "    \n",
    "    traing_error_rmse = rmse(X_train, Y_train)\n",
    "    testing_error_rmse = rmse(X_test, Y_test)\n",
    "    \n",
    "    return (traing_error_mape, testing_error_mape, traing_error_rmse, testing_error_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CNN_LSTM_MODEL_RMSprop\n",
    "def cnn_on_lstm_rms_model(CONV2D_1_DEPTH, CONV2D_2_DEPTH, CNN_DP, #CNN archi related\n",
    "                          LSTM_DP, LSTM_UNITS, #LSTM archi related# RMSprop related\n",
    "                          inputshapes, loss_fnc):\n",
    "    model = Sequential()\n",
    "    # add two time-distributed convolutional layers for feature extraction\n",
    "    model.add(TimeDistributed(Conv2D(CONV2D_1_DEPTH, (3, 3), activation='relu'), input_shape = inputshapes))\n",
    "    #model.add(TimeDistributed(Conv2D(16, (4, 4), activation='relu')))\n",
    "    model.add(TimeDistributed(Conv2D(CONV2D_2_DEPTH, (5, 5), activation='relu')))\n",
    "\n",
    "\n",
    "    # extract features and dropout \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Dropout(CNN_DP))\n",
    "    #print(123123123)\n",
    "    # input to LSTM\n",
    "    model.add(LSTM(LSTM_UNITS, return_sequences=True, dropout=LSTM_DP)) #input_shape=(5, 512), \n",
    "    #print(123123123)\n",
    "    # classifier with sigmoid activation for multilabel\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "    #model.add(TimeDistributed(Dense(1, activation='tanh')))\n",
    "\n",
    "    # compile the model with binary_crossentropy loss for multilabel\n",
    "    #Rmsprop = keras.optimizers.RMSprop(lr=RMSprop_LR, rho=RMSprop_RHO, \n",
    "    #                                   epsilon=None, decay=RMSprop_DECAY)\n",
    "    #model.compile(optimizer= Rmsprop, loss = loss_fnc)\n",
    "    #keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    model.compile(optimizer= 'rmsprop', loss = mape_loss)\n",
    "    # look at the params before training\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global vars time step\n",
    "ts = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1- Hospitilaty  \n",
    "\n",
    "   ### (3, 64, 64, 0.1, 0.1, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4459,) (7, 7, 2)\n"
     ]
    }
   ],
   "source": [
    "## hos_df = pd.read_csv('3_hospitality_13months.csv').iloc[:, 1:8]\n",
    "\n",
    "neighbours_index_image_arys = cpik.load(open(\"3_hospitality_13months.pkl\", \"rb\" ))\n",
    "hos_df['neighbours_index_image'] = neighbours_index_image_arys.tolist()\n",
    "hos_df['neighbours_index_image'] = hos_df['neighbours_index_image'].apply(lambda x : np.array(x))\n",
    "print(hos_df['neighbours_index_image'].shape, hos_df['neighbours_index_image'].iloc[0].shape)\n",
    "ts = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up dateset\n",
    "X_train, Y_train, X_test, Y_test= build_dl_dataset_by_ts(hos_df, ts)\n",
    "X_train = np.array(X_train)\n",
    "X_train= np.nan_to_num(X_train)\n",
    "\n",
    "Y_train = np.array(Y_train)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0],Y_train.shape[1],1)\n",
    "Y_train= np.nan_to_num(Y_train)\n",
    "\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test= np.nan_to_num(X_test)\n",
    "\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0],Y_test.shape[1],1)\n",
    "Y_test= np.nan_to_num(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3087, 3, 7, 7, 2) (3087, 3, 1) (343, 3, 7, 7, 2) (343, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "ind_train = X_train.shape[0]\n",
    "ind_test = X_test.shape[0]\n",
    "ind_2= X_train.shape[1]\n",
    "\n",
    "for a in range(ind_train):\n",
    "    for b in range(ind_2):\n",
    "        scaler.fit(X_train[a,b,:,:,1])\n",
    "        X_train[a,b,:,:,1] = scaler.transform(X_train[a,b,:,:,1])\n",
    "        \n",
    "for c in range(ind_test):\n",
    "    for d in range(ind_2):\n",
    "        scaler.fit(X_test[c,d,:,:,1])\n",
    "        X_test[c,d,:,:,1] = scaler.transform(X_test[c,d,:,:,1])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputshapes = X_train.shape[1:]\n",
    "\n",
    "\n",
    "cnnlstm = cnn_on_lstm_rms_model(128, 64, 0.1, 0.1, 64, inputshapes, mape_loss)\n",
    "cnnlstm.fit(X_train, Y_train, batch_size = 64,epochs=50,verbose=0) #\n",
    "\n",
    "# mape train\n",
    "Y_train_pred = cnnlstm.predict(X_train)\n",
    "Y_train_pred = Y_train_pred[:,-1,:].flatten()\n",
    "Y_train_true = Y_train[:,-1,:].flatten()\n",
    "cnnlstm_train_mape = mape(Y_train_pred, Y_train_true)\n",
    "\n",
    "# mape test\n",
    "Y_test_pred = cnnlstm.predict(X_test)\n",
    "Y_test_pred = Y_test_pred[:,-1,:].flatten()\n",
    "Y_test_true = Y_test[:,-1,:].flatten()\n",
    "cnnlstm_test_mape = mape(Y_test_pred, Y_test_true)\n",
    "\n",
    "\n",
    "# rmse train\n",
    "Y_train_pred = cnnlstm.predict(X_train)\n",
    "Y_train_pred = Y_train_pred[:,-1,:].flatten()\n",
    "Y_train_true = Y_train[:,-1,:].flatten()\n",
    "cnnlstm_train_rmse = rmse(Y_train_pred, Y_train_true)\n",
    "\n",
    "# rmse test\n",
    "Y_test_pred = cnnlstm.predict(X_test)\n",
    "Y_test_pred = Y_test_pred[:,-1,:].flatten()\n",
    "Y_test_true = Y_test[:,-1,:].flatten()\n",
    "cnnlstm_test_rmse = rmse(Y_test_pred, Y_test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3798320296296236,\n",
       " 6.294988779286976,\n",
       " 0.01852283459217138,\n",
       " 0.0351569979997835)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnlstm_train_mape, cnnlstm_test_mape, cnnlstm_train_rmse, cnnlstm_test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnlstm.save('cnnlstm_retail(128, 64, 0.1, 0.1, 64).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
